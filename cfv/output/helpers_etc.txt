From helpers import ~
'jmport numpy_helpers_8451 as npnelp
import inspect
import types
or.
class Pairs2Groups(object):
def _ init (self,
pairs: pd.DataFrame,
items: typing.Optional[pd.DataFrame} = None,
item_id_col: typing.Optional(typing-Union[str, typing.List[str]]] = None,
pair_score: typing. Optional [str] = None,
initial_group: typing.Optional(str) = None,
pre_grouping_func: typing.Optional[typing.Callale[.,., So01)) = None,
post_grouping_ func: typing.Optional[typing.Callable[..., Bool)) = None,
grouping_func: typing.Optional[(typing.Callable[..., tool]} = None,
missing pair_score: typing.Optional[float} = Nome,
item_cols: typing.Optional[typing.Union[{str, typing.List({str]})) = None,
""kwargs) -> None:
Inputs:
Pairs - Pandas OF,
Initialize Pairs2Groups object, check and process inputs.
item pairs used as the basis for amalgamating items into groups.
DataFrame should have 2 columns (or sets of columms) containing identifiers
of the two items to which each row relates, suffixed with 1 and _2. '
| items - Pandas DF, optional, list of items to be amalgamated into groups, DataFrame
i should contain identifier column(s) matching those in pairs (without suffixes).
.
(e.g. if items are identified by 'prod_id', the pairs OF should contain 'columns, '
------------------------------------------------------------------------------------------------------------------------
(e.g. if items are identified by 'prod_ id', the pairs DF should contain columns
*prod_id 1" and 'prod_id 2 , and the items DF should contain 'prod_id'.)
If provided, must contain all of the item IDs that occur in the pairs DF.
If not provided, a unique list of item IDs is generated by deduping the pairs DF.
item_ia_col - Str or list of str, optional, name of column(s) used as item identifiers in
pairs DF (with added suffixes) and items OF.
made to deduce this value from input data.
pair_score - Str, optional, name of column in pairs OF containing values indicating the
strength/quality of each pair. Pairs are assumed to have been provided in
the correct order for processing - i.e. no automatic sorting is done and
pairings are processed in the same order as provided.
initial_group - Str, optional, name of column in items OF describing any pre-existing
grouping of items. If provided, the grouping process will build on
these groupings; otherwise, each item will begin in its own group.
grouping func - Callable function, optional. If prowided, replaces the default
self.grouping iteration method, enabling comtral over whether pairings are
applied based on criteria. The default fumction unconditionally applies
all pairings in tne pairs OF
pre_grouping func - Callable function, optional.
If not provided, an attempt is
If prowides, replaces tne placeholder
self.pre_grouping method, enadling custom steps to be executed before
a new set of grouping iterations.
post_grouping func - Callable function, optional. If prowided, replaces the placeholder
self.post_grouping method, enabling custom steps to be executed after
grouping iterations have completed.
Float, optional, determines what happens when 2 items would be
Joined indirectly by another pairing, but pairs DF does not contain
a@ row for those items. If prowided, sucn a pair is treated as having
score = this value, If not provided, the pair is not evaluates at all
@issing_pair_score -
------------------------------------------------------------------------------------------------------------------------
score = this value.
If not provided, the pair is not evaluated at all
and can be freely joined as an indirect result of other pairings.
item_cols - Name of additional column(s) in items used as item identifiers in
pairs DF (with added suffixes) and items OF.
If not provided, an attempt is
made to deduce this value from input data.
kwargs - Any other key/value pairs provided are added as attributes; this enables passing
of additional parameters used in custom versions of self.grouping_ iteration
  Verify items input against pairs, or create from pairs; identify item ID column names
items, idcols = get_items_from_pairs(pairs=pairs,
items=items,
item_id_col=item_id col)
|   Populate self.item_ia from items[idcols}
self.item_ia =
= items([{c.replace('_N', '") for   in idcols))
if item_cols 1s None: ~ 
item_cols = []
elif isinstance(item_cols, str):
item_cols = {item_cols]
for c in item_cols:
assert c in items.columns
setattr(self, f'item_{c)', items(c].values)
  Populate self.initial group from ixemalAnitialaroyp)) oF or set default starting values
if initial _group is not None:
assert initial group in items.columns
= mp.unique(items{initial group))
, SOM -Anitial_group = items(initial group)\
------------------------------------------------------------------------------------------------------------------------
self.initial_group = items[initial_group]\
. -map(dict(zip(ugps, np.arange(ugps.shape[@]))))\
.values.astype(int)
assert self.initial_group.shape[ @] == self. item_id.shape[@]
else:
self.initial_group = np.arange(self.item_id.snape[@])
# Add additional columns to self from items, per ites_cols parameter
self.item_data_names = ([] if item_cols is None
else [item_cols] if isinstance(item cols, str)
else item_cols)
for c in self.item_data_names:
assert   in items.columns
setattr(self, f'item_{c}*, items(c}).values)
# Populate self.pair_ids, converting item IDs in each pair to indices
pair_ids = paired_item_join(pairs([c.replace("_N*, f*_{j*1}")
for c in idcols for j in range(({"_N" im c)+1)])
.reset_index(drop=True).reset_ingex{},
self. item _id.reset_index(),
item_id_coleitem id col)
assert np.all(pair_ids.index == pair_ids[ 'index' })
self.pair_ids = np.append(pair_ias[['inaex_1 , "index 2 )).walues, [[-1, -1]}], axis=@)
_ # Populate self.pair_scores if specified, otherwise create placeholder
| Af pair_score is not None;
| Self .pair_score = np.append(pairs(pair_score).velues.astype(float),
2   [missing pair score or -1.})
------------------------------------------------------------------------------------------------------------------------
[missing _pair_score or -1.])
assert np.sum(np.isnan(self.pair_score)) == @
else:
self.pair_score = np.ones(self.pair_ids.shape[@}+1)
self.pair_score[-1] = missing_pair_score or -1.
  Flag whether to ignore or process missing pairs
self.ignore_missing_ pairs =
missing pair_score is None
# Map locations of each pair within self.pair_ids for fast lookup during joining loop
self.locs_grid = dict()
for n, (i, j) in enumerate(self.pair_ids[{:-1]):
if i in self.locs_grid.keys():
self .locs grid{ij[jj =n
else:
self.locs_grid[i] = {j: n}
# Grouping function - If passed, replace default self. grouping iteration
if grouping func is not None:
assert callaple(grouping_ func)
assert nasattr(grouping func, *
assert 'self' in inspect.getarg
setattr(self,
code__*)
(grouping func, code). args
'grouping iteration',
types .MethodType(grouping func, self)) |
  Pre-grouping function - If passed, replace default self.pre_groupi
uf pre grouping func is not None: - "
------------------------------------------------------------------------------------------------------------------------
if pre_grouping func is not None:
assert callable(pre_grouping_func)
assert nasattr(pre_grouping_func, "_code__")
assert 'self' in inspect.getargs(pre_grouping func._code__).args
setattr(self,
"pre_grouping',
types.MethodType(pre_grouping func, self))
# Post-grouping function - If passed, replace default self.post_grouping
if post_grouping_runc is not None:
assert callable(post_grouping_ func)
assert hasattr(post_grouping func, '_code_")
assert 'self' in inspect.getargs(post_grouping func. code ).args
setattr(self,
  "post_grouping',
types .MetnodType(post_grouping func, self))
  Ago additional kwargs as attributes
for k, v in kwargs.items():
  setattr(self, k, v)
# Initialize "working" preys
self.reset()
| Gef neset(self) -> None:
* Reset self.group, self.pair_osflag to initial values.
al
------------------------------------------------------------------------------------------------------------------------
self.group = np.copy(self.initial_group)
self.pair_osflag = self.group(self.pair_ids[:, 6)) != self.group[self.pair_ias[:,
1))
def do_grouping(self,
reset: bool = True) -> None:
Reset to original state, execute grouping process.
if reset:
self .reset()
self.pre_grouping()
while True:
if not self.grouping_iteration():
break
Code for post-grouping tasks.
This is the default version, which does nothing.
It can be replacea by a custom method via the post_grouping func parameter.
pass
def
grouping_iteration(self) -> bool:
Code for 2 single grouping iteration.
This is tne default version, which applies all pairings unconditionally.
It can be replaced by a custom method via the grouping func parameter.
Updates:
self.group - NumPy array, cumulative results of grouping.
self.pair_osflag - NumPy array, boolean flags corresponding to sel*.pair_ids.
Identifies "outstanding" pairs, i.e. pairs mot yet evaluated/actioned.
Returns:
Bool - Grouping process will stop iterating when this fumctiom returns False.
  Get mext item pair to try to join, identify all pairings that would occur indirectly
maybe join gps = np.sort(self.group(self.pair_ids[{self.pair pointer})})
@aybe_join_items =
= [np.arange(self.group. snape[Q})(self.group == g) for g in maybe_join_gps)
  Apply pairing (no conditions)
self .group[maybe join_items({1]] = maybe_join_gps(@)
| @ Mark Uhese pairings as no longer outstanding
self .pair_osflag(self.get_pair_ix(maybe join_items)]   False |
1 '
as .
Lexile
------------------------------------------------------------------------------------------------------------------------
@ Update self.pair_pointer, point to next pair where self.pair_osflag = True (if one exists)
self.pair_pointer = np.argmax(self.pair_osflag[self.pair_pointer+1:]) + self.pair_pointer + 1
# Return False if no more outstanding pairs
return self.pair_osflag[self.pair_pointer)
def get_pair_ix(self,
items: typing.List({np.ndarray)) -> np.ndarray:
Given a list of 2 arrays of item indices, identify all item pairs between list @ and list 1 and
return
indices indicating position of that pair in self.pair_ids (amd corresponding scores in
self .pair_score) .
pix = np.array({[il, 12) for 11 in items( ] for 12 im items{1) if i2 be i2)}).astype(int)
pair_ix = np.array(({self.locs_grid.get(il, {}).get(i2, -1) for il, 12 in pix]).astype(int)
filt = pair_ix == -1
pair_ix[filt] = np.array([self.locs _grid.get(i2, {)).get(il, ~1) for il, i2 in pix
{filr}}) .astype(int)
E if self.ignore_missing pairs:
pair_ix = pair_ix{pair_ix I= -1]
return np.unique(pair_ix)
 
_ Set Bet aroups(cls,
------------------------------------------------------------------------------------------------------------------------
| def get_groups(cls,
pairs: pa.DataFrame,
items: typing.Optional[pd.DataFrame] = None,
item_id_col: typing.Union(str, typing.List(str]] = None
pair_score: typing.Optional[str] = None,
initial_group: typing.Optional[str] = None,
pre_grouping func: typing.Optional[typing.Callable[..., bool]} =
post_grouping_func: typing.Optional[{typing.Callable[..., bool}] =
grouping func: typing.Optional[typing.Callable[..., bool}} = None,
missing _pair_score: typing.Optional[float] = None,
item_cols: typing.Optional(typing.Union[str, typing.List[{str]]] = None,
""kwargs) -> pd.DataFrame:
None,
None,
Return resulting groups for specified pairs and other inputs
# Initialize class object and process inputs
op} =
cls(pairs,
items,
item_id_col,
pair_score,
initial group,
pre_grouping func,
post_grouping func,
grouping func,
missing pair_score,
item_cols,
**kwargs)
------------------------------------------------------------------------------------------------------------------------
  Execute grouping from specified inputs, return results
0b} .do_grouping()
return pd.concat((ob}.item_ia,
pd.DataFrame({ 'group': obj.group})),
axis=1)
def get_items_from_pairs(pairs: pd.DataFrame,
items: typing.Optional(pd.DataFrame} = Mone,
item_id_col: typing.Optional(typing.Union[str, typing.List[str]]] = None)\
-> typing.Tuple[pd.DataFrame, typing.List({str]]:
Create items DF and/or list of item ID columns from pairs OF, or werify existing items DF and/or
list of item ID columns for consistency against pairs OF. Pairs impwt must have at least one pair
yp Of columns labelea "{item_id) 1  and '{item_id)_2 , with "item_ig" as the item ID. Other non-
\suffixed
columns may also be needed so that pairs DF does not contain @uplicate item ID pairs.
Inputs:
pairs - Pandas DF, pairs data
items - Pandas DF, optional, list of items being joined via pairs. If None, created from pairs.
Atem_ig_col - Str or list of str, optional, names of columns used to reference items.
Returns:
: , 1. Pandas DF - Items DF, copied from input or created from pairs.
7] 2. List of str - Item ID column names, copied from input or created from pairs.
  Create items DF from pairs input
Sascee aks Ses oe Bek Le ee - - ~
------------------------------------------------------------------------------------------------------------------------
@ Create items DF from pairs input
pairs cols = [f'{i{:-2]}_N' if i[-2:] in('_1', '_2 ) else i for i in pairs.columns]
pairs_cols = [c for nc, c in enumerate(pairs_cols) if pairs_cols.index({c) == nc]
items_from_pairs = pd.concat([pairs{[c.replace('_N', f'_{i+1}") for c in pairs_cols]]}\
.rename(columns=(c.replace('_N', f _{i-1}"):
c.replace("_W",  "*)
for c in pairs_cols if "_N" in c})
for i in range(2)})
# If item_id_col not specified,
if item_ia_col is None:
identify from items/items_from_pairs DF; otherwise verify input
poss_cols =
[  for c in pairs_cols
if (True if items is None else c.replace(*_M", ) im items.columns)
and (c.replace('_N', '') in pairs.columns
or (c.replace('_N', '_1 ) in pairs. columns
and c.replace('_N', *_2* ) in pairs.columms))
item_id_col = [c for c in poss_cols if c.endswitn("_N")}}
| assert len(item_id_col) >   
poss_cols = np.array([c for c in poss_cols if not  .endsmita("_N ))})
mn = poss _cols.shape[6]
' Atem_id_col_p = [c.replace(*_N', f*_{jel}")
for c in item_id col for } in range(2)}
colfl = np.unpackbits(np.arange(2 ** nn, dtypeenp.uint ))\
-Peshape(2 "* nn, -1)[(*, -nn:
GOlfl = colfl{nphelp.get_sort_ix(np.sum(colfl, axis=l)))
for   in colfl:
if pairs[(*item_ia_col_p, 'poss _cols(c.astype(bool))))\
i +@rop_duplicates().shape(@] = pairs. shape[@}:
------------------------------------------------------------------------------------------------------------------------
.drop_duplicates().shape[@] == pairs.shape[@):
item_id_col = item_id_col + poss_cols(c.astype(bool))}.tolist()
break
else:
if isinstance(item_id_col, str):
item_id_col = [item_id_col]
item_id_col = [i if i in pairs.columns else f'{i)_N'
for i in [i.replace('_N', '') for i in item id _col}]
assert any({i.endswith(*_N') for i in item id _col})
assert min({i.replace('_N', f'_{j+1}') in pairs.columns
for i in item_ia_col
for j in range(2)])
assert peirs[[c.replace("_N', f'_{j+1}') for   im item_id_col for j im range(('_N" in c)+1)}]\
.drop_duplicates().shape[ ] == pairs.snape[@)
  If items was specifiea, cneck values vs pairs; otherwise retwre columes from items_from_pairs
  that have consistent values for each item as identified by item_id_col
cols = {c.replace('_N', '') for   in item id col}
if items is not None: :
assert all({c in items.columns for   in cols)})
assert items _from_pairs.merge(items(cols), now= left', omecels, indicator="ind")\
-query("ind != 'botn'").snape[6] == @
assert items[cols}.drop_duplicates().shape(@} == items. shape[@])
else:
| BAtemid = items _from_pairs.drop_duplicates(sudset#cols).shape[@)
items   items_from_pairs([*cols,
*[c for   in items from pairs. columns
if not   in cols
------------------------------------------------------------------------------------------------------------------------
if not   in cols
and items_from_pairs.drop_duplicates(subset= (acarse c])).shape[@)
== n_item_id]})\
.drop_duplicates(subset=cols)
return items.reset_index(drop=True), item_id_col
def to_lway pairs(pairs: pd.DataFrame,
item_id_col: typing.Union[str, typing.List[{str}]) > pd.DataFrame:
Given a Panaas DF of pairs with 2 rows per pair (one for each direction), reduce to a DF with
one row per pair and ID1 < I02).
Inputs:
pairs - Pandas OF, pairs data
item_ia_col - Str or list of str, used to label item ID column(s)
Returns:
Pandas DF
af isinstance(item_id_col, str):
item_id_col = [item_id col]
Atem_ic_col = (i if 1 in pairs.columns else f'{i)_N' for i in [A.replace(*"_N*
\item_Ad_col)}
, *") for i in
assert min(({i.replace('_N', f'_{jrl}') in pairs.columns for i im item_id_col for j in range(2)))
idcols = {j for i in item_id col
for 4 in ({i.replace('_N',
f'_{kel)') for k in range(2))
od if i.endswitn('_N') else [(4)))
------------------------------------------------------------------------------------------------------------------------
  if i.endswith('_N') else [i])}
out_pdf = [pd.concat((pairs[idcols},
pairs[idcols].rename(columns={i: f*{i[:-2]}_{2-j} 
for j in range(2)
for i in idcols if i.endswitn(f*_{j+1} )})))\
.query(_oneway_query([i for i in item_id_col if i.endswith("_N")]))\
.drop_duplicates().reset_index(drop=True) }
att_cols = [c for c in pairs.columns if not   in idcols]}
paired_att_cols = [c for c in {c[{:-2]) for c in att_cols}
\ if min({cc in att_cols for cc in [f*{c}_1 , f  {e}_2 }]}))
for c in paired _att_cols:
out_pdf += [out_pdf(@].merge(pairs, how="left*, ommidcols){[f*{c}) 1 , f {c}_2 ]]]
ix = out_pdf[-1].query(f"{c}_1.isna()"). index
| Af ix.shape[ }:
fillvals = out_pdf[0].reset_index().loc{ix}\
-merge(pairs.rename(columnae{i: f  {if:-2}}_{2-j}*
for j in range(2)
for 1 in idcols if f.emdseith(f'_{j+1}")})
-rename(columms={f*{c}_{j+1}*:   (c}_{2-j} 
for J im range(2))),
| qurt_paf(-1) Berenstain eek anes P
: ~1). , .  )2")}) = s . oe A
(gebeeticmbe patra tivadal 3 21) [lf feda", CR 21
att_cols.remove(cec)
| Len(att_cols):
  ca aid += [pairs(att_cols])
------------------------------------------------------------------------------------------------------------------------
out_pdf += [pairs[att_cols]]
out_pdf = pd.concat(out_pdf, axis=1).drop_duplicates().reset_index(drop=True)
assert out_pdf.shape[@] == out_pdf[{idcols}].drop_duplicates().shape[@]
return out_pdf
def _oneway_query(paired_idcols: typing.List(str]) -> str:
Recursively generate query to filter pairs to one-way only.
cond = #"{pairea_idcols(@].replace('_N', "_1 ))   (paired_idcols[@}.replace("_N", "_2 )}~
if len(paired_iacols) == 1:
return cong
else:
return *"{cond} or"\
+ f" ({paired_idcols[@).replace('_N*, '_1 )) +   {paired_idcols[@).replace('_N*, "_2 )}"\
' +    and (getfirst(paired idcols[1:})))*
(Bef to_2uay_pairs(
Satie
pairs: pd.DataFrame,
a
item_id_col: typing.Union(str, typing.List(str}}) => pd.OataFrame:
_ Given a Pandas DF of pairs witn one row per pair (i.e. mot two-directional),
expand to a OF with 2 rows per pair, one for each direction,
inputs;
= pairs - Pandas OF, pairs data
------------------------------------------------------------------------------------------------------------------------
pairs - Pandas DF, pairs data
item_id_col - Str or list of str, used to label item ID column(s)
Returns:
Pandas OF
ane ve
if isinstance(item_id_col, str):
item_idg_col = [item_ia_col]
item_ig_col = {i if i in pairs.columns else f'{i}_N' for i in [i-replace("_N',  2)) foc io in
item_id_col}]
assert min([i.replace('_N', f'_{j+1}') in pairs.columns for i in item_id_col for j in range(2)))
idcols = [j for i in item_id_col
for j in ({i-replace('_N', f'_{k*1}") for & im range(2))
  if i.endswith('_N') else [i]}))
out_pdf = [pd.concat((pairs[idcols],
pairs[idcols].rename(columns={i: f  (i(:~2))_{2>3,
for j in range(2)
for 1 in idcols if i.endswith(f"_{j+1} )})))
-reset_index(drop=True))
att_cols = [c for c in pairs.columns if not   in ideols}
| paired_att_cols = [c for c in {c{:-2]) for   in att_cols)
if min({cc in att_cols for ce in [f*{e)_a ,   {cp_2 ))))
ki Dalgt tabied att_cols: 7
ied = [pd.concat((pairs((f' {c}_({j+1)* for j in renmeta) Ui.
pairs((f'(c)_{j+1}* for J in range(2))
-rename(columnse(f'(c) {(jel)*: f  eS (2-3) 
------------------------------------------------------------------------------------------------------------------------
-rename(columns={f'{c}_{j+1}": f'{c}_{2-j} 
for j in range(2)})))
.reset_index(drop=True) }
for cc in [f'{c}_{}+1}' for j in range(2))}:
att_cols.remove(cc)
if len(att_cols):
out_pdf += [pd.concat([pairs[att_cols}]}*2).reset_index(drop=True)]
out_pdf = pd.concat(out_pdf, axis=1).drop_duplicates()
assert out_pdf.shape[@] == out_pdf[idcols].drop_duplicates().shape[@]
return out_pdf
ser paired_item_join(pairs: pd.OataFrame,
item_attrib: pd.DataFrame,
item_id_col: typing.Union[str, typing.List(str})},
indicators: bool = False) -> pd.Dataframe;
aan
Given a Pandas DF of pairs witn columns named '<item id 1>" and "<item_id 2> , join to an
attribute OF containing data by item_id_col, adding _1 and 2? suffixes to added columns
Inputs:
pairs - Pandas OF,
pairs aata
item_attrib - Pandas DF, item attribute data, must contain ID column named to match jean Joh
Atem_id_col - Str or list of str, used to label item ID column(s)
indicators - Bool, if True, add boolean columns 'ind_1* and 'ind_2* to indicate whether ae |
: keys were found in attribute data. Default = False.
------------------------------------------------------------------------------------------------------------------------
keys were found in attribute data. Default = False.
Returns:
} Panaas if isinstance(item_id_col, str):
item_id_col = [item_ia_col] - 
item id_col = [i if i in pairs.columns else f'{i)_N' for i in [i-replace("_N', "') for i in
item_id_col]}] =)
 *_{jr1}') in pairs.columns for i in item id_col for j in range(2)))
assert min([i.replace('_N',
assert min([i.replace('_N', '') in item_attrib.columns for i in item_id_col))
attrib_cols = [c for c in item_attrib.columns
if not (c in [i.replace("_N', '*) for i in item id col})]
r out_pdf = pairs.merge(item_attrib.rename(columns=(*"{i.replace(*_M", **\: d.replace("_N", '_1 ) for i
in item_id_col},
"*({c: #*(c)_1  for c im attrib_cols}}),
how="left',
on=[i.replace("_N', '_1") for i im item id col},
indicator="ind_1* if indicators else False)\
-merge(item_attrib.rename(columns=(**({i.replace({*"_ ",
in item_id_col},
. s*(c: @'(e}_2" for c im attrib_cols}}),
: icreplace("_N*, '_2") for i
how="left',
on=[i.replace("_N', "_2") for i im item id col),
indicator="ind_2  if indicators else False)
af indicators:
j for c in (*ind_1*, 'ind_2'):
out_paf(c) = out_pdf[c].map({'both'; True, 'left_omly': False)})
| netunn out_pdf
------------------------------------------------------------------------------------------------------------------------
me
M51
s
a
[P< Users > 9977209 > git + offer-optimization   sre   utils
Name
@ _int_py
4 azure_utils.py
  campaign_data.py
@ clean _text_cols.py
@ exp_smootning py
~ U Search utils
Date modified
024 12110 PM
4120 PM
241i OPM
mst 1PM
Type
PY File
PY File
PY File
PY File
ee |
------------------------------------------------------------------------------------------------------------------------
#rom helpers import *
from decimal import Decimal as D
def exp_smoothing(df: DataFrame,
cols 'str,
time_col: str,
by: UP a eo aaa typing.List[str}]] = Mone,
alpna: float = 6.3,
fill_nulls: typing.Optional[float] = None) -> OataFrame:
Perform exponential smoothing on values in a DataFrame, with optiom to subdivide using "by" groups.
| Input DataFrame must be unique on by+time_col column values.
Inputs:
df - Spark DF, data with values to be smoothed
col - Str, name of numeric column in OF to be smoothed
time_col - Str, name of column in DF denoting time - order of walwes must match sequence in time
by - Str or List of strings, names of column(s) in OF smose walwes define by groups
alpha - Alpha factor for smoothing. Oefault = 0.3.
1 fill_nulls: Float, optional - if specified, treat null input walwes as this value in smoothing
calculation
(e.g- set ==   to treat null quantities as zeros). If mot specified, the smoothed
\wahue from the
} previous time period is carried forward when a null input appears for that period,
| (ieote: output
: | will still contain nulls for time periods prior to the first non-null input.) |
------------------------------------------------------------------------------------------------------------------------
Spark DF
@ Pivot data time values into columns
df_wide = sqlhelp.pivot_aata(df,
time_col,
col,
by=by,
prefix="p_*)
@ Get list of time columns
timecols = [c for c in df_wide.columns if c.startswith(f*{col})_p_ ))
@ Do exponential smoothing calculations and add smoothed values to of_wide
Of _wide.createOrReplaceTempview( 'df_wide")
df_wide = spark.sql(make_query(timecols,
alpha,
fill_nulls=fill_nulls))
|   Unpivot back to long form table and return
) feturn sqlhelp.unpivot_aata(df_wide,
time col,
f'sm_{col)',
by=by,
prefix='p_*)
fief mee guery(tinecols: typing.List(str),
alpha: float,
SS
------------------------------------------------------------------------------------------------------------------------
alpha: float,
fill_nulls: typing-Optional[ float)
= None,
ten: typing.Optional[int]
= None) -> str:
Generate SQL query used to add smoothed values to df_wide table.
(Recursive, to enable sequential calculation for each time period)
Inputs:
timecols: List of str, names of columns in df_wide containing values for time periods,
in chronological order
alpha: Float, alpha parameter for exponential smoothing
#ill_nulls: Float, optional - replacement value for mulls, as described in
exp_smoothing() docstring.
ten: Int, optional - used within function to control recursion; cam also be specified by user
to limit smoothing to first N columm im timecals
Returns:
String
if ten is None:
= len(timecols)-1
if ten =-.8:
  For first time column, copy input column to smoothed column
query = f"""
SELECT *,
{timecols[tcn}) AS sm_(timecols(ten})}
FROM df wide
------------------------------------------------------------------------------------------------------------------------
else:
# Otherwise calculate smoothed column from current input, previous smoothed column
sm_col_def = smoothing calc(alpha,
timecols(tcn],
#'sm_{timecols[tcn-1]}",
me fill_nulls=fill_nulls)
query = f"""
SELECT) 5
{sm_col_def}
FROM ({make_query(timecols, alpna, fill_nulls, tcn-1))
return * *.join(query.split())
def smoothing calc(alpna: float,
cok; | Strs
prev_col: str,
fill_nulls:
typing.Optional[{int] = None) -> str:
Return case-when statement used to calculate smoothed value for specified column
Inputs:
alpha: Float, alpna parameter for exponential smoothing
col: Str, name of column containing current time period input walue
prev_col: Str, name of column containing prior time period smootnec value
#ill_nulls: Float, optional - replacement value for mulls, as Gescrided in
exp_smoothing() docstring.
Returns:
String
Af fill_nulls is None:
7 Fill_nulls = "NULL"
| return ' * Join([ f  CASE",
f WHEN (prev_col) IS NULL THEN (col)}",
f WHEN NVL({col}, (fill _nulls}) IS NULL THEN {prev_col)", i
wo 7 oe f"ELSE ({alpna) * NVL({col), (fill_mulls))) + ((O   1 )   O(str(alpna))) *
doer c0l)", i
al
be cS _ FPEND AS sm_{col)"))
------------------------------------------------------------------------------------------------------------------------
[/* Users > 9977209 > git + offer-optimization   sre > utils
mf.
we * Name
s @ _init_py
  azure _utils.py
' r campaign_data.py
4 clean_text_cols.py
Tears Chat Files
"   pairs2groups py
'
rated Videos
t
Ls
Le
v0 search
Date modified
2024 12:50 PM
ba4 12) PM
Bau eM
_ __ i _ Pat
Type
PY File
PY File
PY File
a
Woe aes
------------------------------------------------------------------------------------------------------------------------
from helpers import *
from azure.identity import DefaultAzureCredential
from azure.storage.blod import BlobServiceClient
from io import BytesIO
from fnmatch import fnmatcn I
from pathlib import Path
class AzureData(object):
Contains helper methoas for reading data directly from ADLS storage mithowt using Spark.
- AzureData
.download_to_dbfs(adls_path, dbfs_path) ~ Copy file(s) from Azure storage to DBFS
- AzureData.delete(adls_patn) - Delete file(s) at adis path
- AzureData
- AzureData
-parquet_to_pdf(adls_path) - Read parquet date at adls_pat  and returns as Pandas DF.
-pdf_to_parquet(pdf, adls_path) ~ Write Pandas OF to parquet file at adls_path.
token_crecential = DefaultAzureCredential()
| def init (self,
adls path: str) -  None:
Define attributes for input ADLS path and (optionally) Slo  names.
Inputs ;
adls_patn - Str, ADLS location "abfss:// ... *
name - Str with wildcards (*), blob names to include in object
; Defines attributes:
Bw adls path - from input
f catr - Data container from adls path
------------------------------------------------------------------------------------------------------------------------
cntr - Data container from adls_path
sa_env - Storage account and env from adls_path
path - Location of data from adls_path
container_client - ContainerClient object for interacting with data
folders - List of str, names of folder blobs under adls_path
  plobs - List of str, names of non-folder blobs under adls_path
self.parse_adls_patn()
self.get_az_container_client()
self.get_blob_lists()
self.adls_path = adls_path[:-1] if adls_path.endswith('/") else adls_path
f@fc lassmethod
def uploaa_from_dbfs(cls,
adls_path: str,
dbfs_path: str) -> None:
Write copy of data at specified location on DBFS to specified ADLS location
Inputs:
"|
| adls_patn - Str, ADLS location "abfss:// ... yh)
dbfs_path - Str, DBFS location "/dbfs..."
Returns:
j
ia! Nothing
obj = cls(adls_path)
obj.do_upload_from_dbfs(dbfs_path)
@classmethoa
def delete(cls,
adls_path: str) -> None:
Delete all blobs at specified ADLS location (data Dloms first, them folders)
Inputs:
adls_path - Str, ADLS location "abfss:/,
Returns:
_, Nothing
  cls(adls_patn)
pacar
------------------------------------------------------------------------------------------------------------------------
@classmethoa
def parquet_to_pdf(cls,
adls_path: str) -> None:
Return parquet data at specified ADLS location as a Pandas OF
Inputs:
adls_patn - Str, ADLS location "abfss:// ...
Returns:
Pandas DF
obj = cls(adls_path)
return obj.go_parquet_to_pdf()
@classmetnod
def pdf_to_parquet(cls,
pdf: pa.DataFrame,
adls_path: str) -> None:
Write the contents of a Pandas DF to @ parquet file at specified ADLS location
Inputs:
pdf - Pandas DF
adls_patn - Str, ADLS location "abfss://
Pandas OF
|
f = cls(adis patn)
  289. ~t _parquet (pat)
------------------------------------------------------------------------------------------------------------------------
! obj.do_pdf_to_parquet(pdf)
def parse_adls_patn(self) -> None:
Parse Storage Account, Container and file path from a full ADLS path
Inputs:
None
Defines attributes:
cntr - Name of container
sa_env - Name of Storage Account including env
path - File name including full path
Returns:
Nothing
path_parts = self.adls_path.replace('abfss://", '
-replace('@',   *) \
-replace(*.dfs.core.windows .met/'
.split()
| self.cntr, self.sa_env = path_parts[:2)
| self.path = patn_parts(2) if len(patnh_ parts) > 2 else '
get_az_container_client(self) ->  None;
Create the ContainerClient object used to interact with the storage account container
'Inputs;
None
 Oefines
nt S$:
5, Somtalner_client - ContainerClient object
------------------------------------------------------------------------------------------------------------------------
def
container_client - ContainerClient object
Returns:
Nothing
blob_service_client = BlobServiceClient(
account_url=f"nttps://{self.sa_env}.blob.core.windows .net",
credential=self.token_credential
)
self.container_client = blob_service_client.get_container_client(self.cntr)
get_blob_lists(self) -  None:
Get list of plobs at adls_ path, split into data blots and folder blobs
Inputs: -
None
| Defines attributes:
folders - List of str, names of folder Dlods under adls path
blobs - List of str, names of non-folder blobs under adls path
Returns:
Nothing
wan
Af *** in self.patn:
base path = self.patn.split('/")
ee = */".join(base_patn[:min((i for i, J im enumerate(base_path) if '** im j))))
base_path   self.path
iho = (D.name for b in self.container_client. list blods(name_starts iia
if (b. pee =- base_path and D.name a= self. patn)
------------------------------------------------------------------------------------------------------------------------
if (b.name == base path and b.name == self.path)
or fnmatch(b.name, self.path)
or fnmatch(b.name, f"{self.path}/*")]
self.folaers, self.blobs = self._parse_blobs(base_path, blob_list)
@staticmethod
def _parse_blobs(path, blob_list):
Identify data blobs (vs folder blobs) in blob list, returm list of folders and gata
Inputs:
path - Str, ADLS location from which blob list was generated
blob_list - List of Str
Returns:
1. List of str, patns of folders below input path
2. List of str, paths of data blobs below input path
sublist = [f"{patn}/{b[len(patn)+1:].split('/*)[@))}".strip 
for b in Dlob_list if b.startswith(f"{patn)")?
if len(sublist) - (path in sublist) re @:
y return [J], sublist
| else:
| folders = [path] * (path in sublist)
blobs = [] ;
for p in set(sublist):
Af p == path:
continue
ff, bb = AzureDate. parse _blobs(p, blob list)
| folders += ff
blobs.
------------------------------------------------------------------------------------------------------------------------
folders += ff
blobs += bb
return sorted(folaers), sorted(blobs)
def do_download_to_dbfs(self
dbfs_path: str) -> None:
Write copy of data at ADLS location to specified location on O8FS-.
check_dbts_path(dbfs_path)
scs = len(os.patn.dirname(self.patn))
for f in self. folders:
check_dbfs_patn(os.path.join(dbfs path, f(scs:}.strip(
for b in self.blobs:
with open(os.path.join(dbfs_path, b[scs:}.strip(', j), mode="wm") as f:
|  .write(self.container_client. download blob(>).read{>}*
do_upload_ from_dbfs(self,
dbfs_patn: str) -> None:
def
Write copy of data at specified location on DBFS to ADLS location.
| @ sert not '** in self.path
get_flist = lampaa x: [f for p in Path(os.path.dirname(x)).glod(os. path. basename(x) or Ei
| for f in (get_flist(f*(os.patn.dirname(x))}/{p.name}/~)
if p.is_dir() else
(f  (os. path, dirname(x) i
Apfx   Len(os.path.dirname(dbfs_path))+1 Siac Dili re
------------------------------------------------------------------------------------------------------------------------
lpfx = len(os.path.dirname(dbfs_path) )+1
for b in [f[lpfx:] for f in get_flist(dbfs_patn)]:
with open(os.path.join(os.path.dirname(dbfs_path), b), mode="rb") as f:
self.container_client.upload_blob(os.patn. join(self.path, b),
f.read(),
overwrite=True)
self.get_blop_lists()
do_delete(self) -> None:
Delete all blobs at ADLS location (data blobs first, them folders)
self.container_client.delete_blops(*self.blobs)
self .container_client.delete_blobs(*self. folders)
self.get_blob_lists()
parquet_blop_to_pdf(self,
blob):
Read a single parquet blob and return as Pandas Oataframe
Gownloader = self.container_client.get_blob_cilent(dlod=blad). download blob()
buffer = Bytes10()
downloader .readinto(buffer)
return pd.read parquet(buffer, engine='pyarrow')
Get G0 _parquet_to_pdf(self) -  None:
------------------------------------------------------------------------------------------------------------------------
def do_parquet_to_pdf(self) -> None:
Return parquet data at ADLS location as a Pandas DF
parquet_blobs = [b for b in self.blobs if b.endswith(" .parquet')]
if len(parquet_blobs)   @:
return pd.concat({self.parquet_blob_to_pdf(b)
for b in parquet_blobs]).reset_index(drop=True)
else:
return None
do_pdf_to_parquet(self,
pdf: pa.DataFrame) -> None:
Write the contents of a Pandas DF to a parquet file at AS location
assert not """ in self.patn
buffer = BytesI0()
pdf.to_parquet(buffer, indexeFalse)
self .do_delete()
f self container_client.upload_blob(f"{self.patnh)/data, parquet", J
buffer. getvalue(), '
r overweite=True)
| self get blob lists()
------------------------------------------------------------------------------------------------------------------------
i" Users  g977209 > git > offer-optimization >
wast a
s
pated Videos
rs
u
sre > utils
Name
@ _init_py
@ campaign data py
 
a clean_text_cols.py
 ) exp_smoothing py
4 pairs2groups py
  U   earch utils
024 12:50 PM
meu 1PM
ag eM
------------------------------------------------------------------------------------------------------------------------
{Sears (go7Tans > GR > offerioptimization'> 'sp,  
Name
b grouping
fh monthly
B outputs
a
vo
------------------------------------------------------------------------------------------------------------------------
{> OSDek (C) > Users   9977209   git   Riisricommuzetiet
oh. Fa es
a
Name
| e451
_ git
\ idea
| helpers
Bb notebooks
hb resources
 
gitignore
README md
we
~   Search sffer-optimization
Date modified Type
3 1017 AM File folder'
Gas 2, 1 PM File folder
mY File foider
aaa a
hit
Sa
------------------------------------------------------------------------------------------------------------------------
 OSDisk (C) > Users > g977209 > git > offer-aptimization  
a
Vas)
%
Name
------------------------------------------------------------------------------------------------------------------------
from .std_imports import *
from pyspark.sql import SparkSession
from pyspark.dbutils import DBUtils
from effodata.effoaata import ACDS
from pyspark.sql.dataframe import DataFrame
from pyspark.sql.types import StringType, DoubleType, LongType, StructType, StructField
I
spark = SparkSession._instantiatedSession
dbutils = DBUtils(spark)
acds = None
def get_acds() -  ACDS:
Define global acas object if not previously defined, then reture
| global acds
Af not acdas:
acos = ACDS(spark, use_sample_martsFalse)
return acds 'i
(et dookariat: DataFrame,
R_obs: int = 10,
a a a typing Optional[typing-Union(str, None}] = None) -> pd. DataFrame
ce a
------------------------------------------------------------------------------------------------------------------------
Return first N (default=10) records of Spark DF as Pandas DF (for visual check).
(if n_obs ==  , all records are returned)
If af input is a string, it is assumed to be a Spark SQL view name.
Option to add a condition (as in a Spark filter() or SQL WHERE statement) via cond parameter.
if isinstance(df, str):
query = f"SELECT COUNT(") AS nct FROM {df}"
if isinstance(cond, str):
query += f" WHERE {cond}"
return spark.sql(query).toPandas().iloc(@, @]
else:
if isinstance(cond, str):
return df.filter(cond).count()
else:
i
return df.count()
column_list(df: DataFrame) -> list:
Return list of column names from a Spark OF.
oan
If df input is a string, it is assumed to be 4 Spark SQ wiew mame.
if ininstance(df, str):
return spark.sql(f"SELECT * FROM (df}"). columns
| e@lif isinstance(df, OataFrame):
return of .columns
nem 9 (pbsect': stranetype(),
oe egkagory Str inatye(),
------------------------------------------------------------------------------------------------------------------------
'category': StringType(),
*float32 : DoubleType(),
"float6 4': DoubleType(),
*int32 : LongType(),
*int64': LongType())}
def schema_from_pdf(pdf: pd.DataFrame) -> StructType:
Generate schema for converting a Pandas DataFrame into a Spark OstaFrame-
Inputs:
pdf - Pandas DataFrame
Returns:
StructType schema object
return StructType([StructFiela(i, dt_sch[pdf[(i}.dtype.name}, True) for i im pdf.columns])
pandas to df(pdf: pd.DataFrame) -> Dataframe:
Convert a Pandas DataFrame into
@ Spark Dataframe .
Inputs:
pdf - Pandas DataFrame
Returns:
OF, data from pdf
"=
pager @park.createDataFrame(pdf, schema_from_pdf(pat))
------------------------------------------------------------------------------------------------------------------------
def df -to_parauet (a: Betatrase,
adls_path: str,
write_mode: str = 'overwrite') -> DataFrame:
ous
Write DF to parquet, return DF read from parquet
(Use to save data at "checkpoint" during a multi-step process.)
Inputs:
df - Spark DF.
adls_path - String like 'abfss://...", location to write to.
write_mode - String, 'overwrite' to create new parquet,
- n append' to append to existing parquet
Returns:
| Spark DF, read from parquet after saving
assert write_mode in('overwrite', 'append')
of write .mode(write_mode).parquet(adls_patn)
meturn spark.read.parquet(adls_ path)
we pandas _to_parquet(pdf: pd.Datoframe,
adls_patn: str,
a write mode: str = 'overwrite') -> Dataframe:
it Convert a Pandas DataFrame into a Spark DataFrame and write it out to parquet.
Inputs:
pdf - Pandas DataFrame
adls_ path - Location to write parquet
write_mode - String, 'overwrite' to create new parquet,
'append' to append to existing parquet
Returns:
Spark DF, read from parquet after saving
return df_to_parquet(pandas_to_df(pdf), adls_path, write_sode-write_mode)
check_dbfs_patn(dbfs_patn: str) -> None:
Check whether tne specified path exists; try to create it using os.maedirs if mot.
Inputs:
dbfs_patn - String, OBFS path.
(Assumed to include the filename if portion after the
last forward-slasnh contains @ period, im which case the filename is ignored).
Nothing
Af pot *.' in dbfs_patn.split('/')(-1]:
dbfs_patn += '/*
| pathdir = 06.path.dirname(dbfs_path)
Af not o5.path.exists(pathdir):
a   -makedirs(pathdir)
------------------------------------------------------------------------------------------------------------------------
{  Users > g977209   git   offer-optimization   helpers
ws
a
Name
A dataen
 @ _ini_py
@} job_helpers,
a misc.py
@ sqihelp py
 ) s1a_impons py
------------------------------------------------------------------------------------------------------------------------
From  $td_imports import *
import copy
class JopContext(object):
def init__(self,
config: typing.Optional(typing.Dict[str, typing-Dict[(str, s str})}] =
""kwargs: str) -  None:
Initialize JopContext object witn named parameters and comfig dict
Inputs:
- config - Config dict, may have multiple levels, e.g.
{"source': {"key1': 'valuel", 'key2": "walwe2", . ..>),
*target': {'key3": 'value3", "key": "walued", ...>}}
Values can contain placeholders demarked Oy square Srackets, with values to
be specified in "*kwargs; e.g. if initialired wit  week= "28228504", and config
contains {'data_out'; 'abfss://patn_to data/[week)/"}, this is updated to
{"data_out': *
abfss://path_to_data/2@22@$@4/"}. Amy key/value pairs with
= unresolved placenolders are removed, raising Cey rror if the key is later
weferenced.)
- "*kwargs - Parameter variable names and values
  Set self.config from input template
new'), sp_setattr__ ('_raw_config',
dict() if config is None else copy.deepeapy(config))
# Set etiriputes from kw   update values in self. pws
ter pk, py An kwargs.items() i
------------------------------------------------------------------------------------------------------------------------
for pk, py in kwargs.items():
_ super().__setattr__(pk, pv)
_ph_repl(self._raw_config, pk, pv)
# Copy self._raw_config without unresolved placeholders to self.config
super().__setattr__("config', _cfg_no_ph(self._raw_config))
def _ setattr_(self,
name: str,
value: object):
Agjusted setattr - when a new attribute value is set, self.comfig is updated
to incluce corresponding placenolder values.
super().__setattr__(name, value)
if nasattr(self, '"_raw_config') and isinstance(value, (str, imt))
_pn_repl(self. _raw_config, name, value)
super().__setattr__('config', _cfg_no_pn(self. caw comfig))
def _ph_repl(
config: typing.Dict[str, typing.Union[str, dict)},
param_key: str,
* param_val; typing.Union[str, int]) => None:
   Replace placeholders with parameter values in config dict.
Inputs:
config - Dict
param_key - Str, text to replace where found in square brackets in config values
param_val - Str or int, text to replace "[param_key]" with.
Return:
Nothing (updates config input)
for ck in config.keys():
if isinstance(config[ck], dict):
_ph_repl(config(ck], param_key, param_val)
elif isinstance(config(ck], str):
config({ck] =
config[ck].replace(f"[{{param_key)}", str(param_wal))
_efg_no_pn(config: typing.Dict[str, typing.Union[str, dict))
>  @ict:
Return copy of config dictionary, omitting values with unreselwed placemolder strings.
Inputs:
config - Dict
i
weturn (k:  fg_no_pn(v) if isinstance(v, dict) else wv
tL for k, v in config.items()
Af mot(isinstance(v, str) and '[' in v))
------------------------------------------------------------------------------------------------------------------------
  Users > g977209   git > offer-optimization   helpers
was)
es
A
Name
A data_ext
@ _init_py
P azure_helpers.py
@ miscpy
  saineip py
@) std_impons.py
~ UO Seich! pen
------------------------------------------------------------------------------------------------------------------------
def sub from dict(value, replace_dict):
Apply a series of text substitutions to input text string according to replace_dict,
a dictionary of strings to replace (keys) and strings to replace them with (values)
  Inputs:
value - Str, input string
replace dict - Dict of str: str
_ Returns:
StrinString
if len(replace_dict) == @:
return value
else:
di = list(replace_dict.items())
return sub_from_dict(value.replace(di[@][@], str(di(@)[1]})), dict(di[1-)))
def file_label(client: str) -> str:
Make file labeling string from input string.
Removes all characters
that are not letters or numbers, replaces spaces/removed letters  ith
underscores and removes leading, trailing and repeated underr-ores)}
Inputs:
str
Returns:
str
td
client_l = [i if (65 <= ord(i) <= 90)
or (97 <= ord(i)  <e 122)
or (48 <= ord(1) <= 57) else  _'
| for 1 in client}
chient_1 = '*.join((i for ni, 4 in enumerate(client 1)
' if i ie '_* or client_i[ni
. 7 A AS
| Petwre client). strip('_")
------------------------------------------------------------------------------------------------------------------------
{  Users   q977209 > git > offer-optimization >  helpers
M5)
Name
es A dataen
sy   _inn_py
5 @) azure helpers. py
fe ted
id @ sqineip py
} @) sta_imparts py
L
rated Videos
La
'
he ieee
~ UO Search \eipers
Date modified Type
024 12:50 PM File foider
O24 17 OPM PY File
et JPM PY File
__) PAA PY File
"Ber Wie. | PY File
"= Bem Py File
------------------------------------------------------------------------------------------------------------------------
J data_ext
@ _init_py
] azure_helpers.py
@ job_heipers py
P] misc.py
Bren.
& s1_impons. py
ETE
------------------------------------------------------------------------------------------------------------------------
from .std_imports import "
from .azure_helpers import "
from .misc import subd_from_dict
'def get_sql_select(column_aefs: typing. List[typing.Union[str, tuple, list}],
replace dict: typing.Dict[str, str] = {}) -> OstaFrame:
Generate text for SELECT statement to extract specified columns using Spark SQL,
applying column renames and decodes as defined in column_defs.
Inputs:
column_defs - List of Str/Tuple
Str or Tuple lengtn 1 (<column>)
- Equivalent to SELECT <column>
Tuple length 2 (<column>, <expr>)
- Equivalent to SELECT <expr> AS <column>.
Tuple length >2 (<column>, (<cond1>, <expri>), ... , Cenpr >,
- Equivalent to SELECT CASE WHEN <condl> THEM <exprd>
Returns:
String
mee
select_lines = []
det = (f"*({k}} ": v for k, v in replace dict. items())
. for   in column_defs:
Af Asinstance(c, str):
~  @ String - take column as-is
~ select lines v= [c]
AU dant me 1:
pie eae
. ELSE cexpr&> END AS <column>.
------------------------------------------------------------------------------------------------------------------------
elif len(c) == 1:
# Tuple/list of len 1 - take column as-is
select_lines -= (c[ )}]
elif len(c) == 2:
@ Tuple/list of len 2 - define column by renaming another column
value = sub_from_dict(c[1], rdct) if '[* in c{1] else c{1)
select_lines += [f"{value} AS {c[@}}")
else:
# Longer tuple/list - CASE statement
raw_whth = [[t] if isinstance(t, str) else list(t) for t im c{1:}]
  (Parse ELSE instruction)
if len(raw_wntn[-1]}) == 1:
elvl = raw_wnth.pop(-1)[ ]
elvl = sub_from_dict(elvl, ract) if "{" in elwl else elwl
else_exp = f" ELSE {elvl)"
else:
else_exp = *
  (Parse WHEN-THEN instructions)
wnen_lines = (]
for wn, th in raw_wntn:
value = sub _from_dict(th, rdct) if '[* im th else tm
when_lines += [f'WHEN {wn} THEN (value}*)
  (Generate statement)
select_lines += [(f"CASE {" *.join(wnhen lines)}{else exp) END AS (c[@)}}")
| Peturn ', *.join(select_lines)
roe
------------------------------------------------------------------------------------------------------------------------
det top_by_partition(df: DataFrame,
cols: typing.Union[str, typing.List[str}],
rank_cols: typing.Union(str, typing-List(str]] = [],
part_cols: typing.Union(str, typing.List({str}} = [J],
keep_rank_cols: bool = False,
n_per_partition: int = 1) -> DataFrame
Get top values for specified columns by partition, ordering as specified.
Inputs:
af - Spark OF
cols - Str or List or Str, columns to get top-ranked values for.
rank_cols - Str or List or Str, columns to rank input roms Sy Before selecting top-ranked ees}
part_cols - Str or List or Str, columns to partition By.
keep_rank_cols - Bool; if True, rank_cols are retained im retureed OF, default = False.
| M_per_partition - Int; number of top-ranked rows to returm per partition, default = 1.
(If set == 0, all values are returned with remking. }
' Returns:
Spark OF
ane
| Af isinstance(cols, str):
\ cols = [cols]
' Af isimstance(rank_cols, str):
4 rank_cols = [rank_cols}
u" isinstance(part_cols, str):
part _cols = (part_cols)
=
"tt -createOrReplacelempview('df')
------------------------------------------------------------------------------------------------------------------------
df .createOrReplaceTempView( 'af' )
query = f""
SELECT {*, '.join(part_cols
cols
+([c.replace('-",
**) for c in rank_cols} if keep_rank_cols else [])
+(['rank_num'] if n_per_partition i= 1 else []}))}
FROM (SELECT m.*,
RANK () OVER ({"" if len(part_cols) == @ else "PARTITION BY"} {", '.join
(part_cols))
ORDER BY {*, ".join([f*({c{1:}} OESC* if c[@] = "-* else c for   in
rank_cols}  
+ cols) ) AS rank_oum
FROM df m)
WHERE rank_num <= {n_per_partition) OR (n_per_partition}    @
return spark.sql(query)
ef cumulative_sum(df: DataFrame,
cols: typing.Union([str, typing. List{str}},
rank_cols: typing.Union[str, typing. List(str}},
ree part_cols: typing.Optional(typing.Union[str, typing.List[str]}]) = Nome) -> maineiania,
Get cumulative sum of column in Dataframe, with options to partition by specified columns
. OF ~ Spark DF
L ,  
_ String or list of strings, column(s) to get cumulative sum of
------------------------------------------------------------------------------------------------------------------------
cols - String or list of strings, column(s) to get cumulative sum of
rank_cols - String or list of strings, columns to sort input rows by before calculating
cumulative sums 5
part_cols - String or list of strings, column(s) to partition by (optional)
Returns:
Spark OF
if isinstance(cols, str):
cols = [cols]
if isinstance(rank_cols, str):
rank_cols = [rank_cols}
if part_cols is None:
part_cols = []
elif isinstance(part_cols, str):
part_cols = {part_cols]
part_cols txt = *, '.join([f"{c}" for c in part_cols))}
partby_txt = f PARTITION BY {part_cols txt)' if len(part_cols_ txt) else '
rank_cols txt = ', '.join((f"{c}" for   in rank_cols})
cum_sel =", *.join([f"SUM({c}) OVER ({partby txt} ORDER BY (ramk_cols txt}) AS cum_(c}"
for c in cols))
| @f.createOrReplaceTempView( 'df' )
jquery  foo
r*,
{cum sel)
FROM of
------------------------------------------------------------------------------------------------------------------------
return spark.sql(query)
def get_modal_value(df: DataFrame,
cols: typing-Union{str, typing.List[{str}],
part_cols: typing.Optional[typing.Union[str, typing.List[{str]]] = None,
weignts: typing.Optional[str] = None) -> DataFrame:
Get modal/most frequent value of column in DataFrame, with options to partition
and/or weight by other specified columns.
Inputs:
of - Spark OF
cols - String or list of strings, column(s) to get sodal walue(s) of
(For multiple columns, returned values will be the most frequentiy-
 - occurring combination of values, not the individwal sede fer each column)
part_cols - String or list of strings, column(s) to partition ey (optional)
weights - String, column to weight input columns By (optional)
Returns:
Spark DOF
wae
af isinstance(cols, str):
cols = [cols]
f part_cols is None:
Disisdsothecen =)
a Asinstance(part_cols, str):
1 ere = [part_cols}
| AC maa ts tone:
------------------------------------------------------------------------------------------------------------------------
return spark.sql(query)
tet gett Bex aur Postercane,
cols: typing.Union{str, typing. List(str]],
part_cols: | typing.Optional[typing-Union{str, typim ist[{str]}]
weignts: typingsOptdonal[str) = Nor ~ lata! 'ame
 - SS
Get mocdalfoost fr quectovalde @f chldon iniDapaFrase) wlth it
"andor weight by @tber. lel cay pal a
i
WaluerespartpBFh..4,
oeis.- String Or tistw stran,
Preturn top_byuparuitioacveruentred,
cccure tse coepleateda,c, vasuees
oradk: Colte'*n datepte lve
' pure_colsepant cola}
- et _sedian value(s: DataFrame,
oe y cols: typing.Union{str, CYPINB samy THRE gy
part_cols: typing.Optional[ typing.Unior
. Syping,list(ste])) = None,
weignts: typing-Optional(str]   None,
car | if_two_middle values; stra '
mean'} ~> DataFrame;
lue of column in Dataframe, with options to partition
f by other specified columns.
'
------------------------------------------------------------------------------------------------------------------------
if weights is None:
weights = '1 
df .createOrReplaceTempView( 'df')
query = f"""
SELECT {  '.join([f"{c}," for   in part_cols]})}
{', *-join([c for c in cols]})},
SUM({weignts}) AS n_datapts
FROM df
GROUP BY {* '.join({f"{c}," for   in part_cols)))
{', "-join([c for   in cols})}
walue_freq = spark.sqi(query)
return top_by_partition(value freq,
- cols=cols,
rank_cols="-n_datapts',
part_cols=part_cols)
def get_median value(df: DataFrame,
cols: typing.Union(str, Ctyping.List{str}},
part_cols: typing.Optional[(typing.Union[str, typing.List({str))) = None,
weignts: typing.Optional[str] = None,
j if_two_ middle values: str = 'mean') -> Dataframe:
one
Get pea value of column in DataFrame, with options to partition
aoa/o weight by other specified columns.
------------------------------------------------------------------------------------------------------------------------
and/or weight by otner specified columns.
Inputs:
df - Spark OF
cols - Str or list of str, column(s) to get median value(s) of
part_cols - Str or list of str, optional, column(s) to partition by
weights - Str, optional, column to weight input columns by
if_two_middle_values - Str, -: defines behavior when two middle values exist.
*mean': mean of the two values is returmed (the standard way).
min' or 'max": the lower or higher walue is returned (used
where an output value that occurs im the data is desired).
Default = 'mean'
Returns: oe
Spark DF
if isinstance(cols
cols = (cols)
if part_cols is None:
part_cols = []
elif isinstance(part_cols, str)
part_cols = [part_cols]
Af weights is None:
weights = '1 
seers
of. pags TR df')y
* * join((f"m.{c}," for   in part_cols)})
joe ri *,join((f AND m.{c) = x.{c)" for   in part_cols)})
[on gehete ange Gdantcols)):
------------------------------------------------------------------------------------------------------------------------
an.
for coln in range(len(cols)):
# Count frequency of values per BY group
query < f7""
SELECT {bytxt}
{cols{coln}},
~ SUM({weignts}) AS n_datapts
FROM df m
GROUP BY {bytxt)
{cols[coln}}
spark.sql(query).createOrReplaceTempView( 'value_cts")
query =   ""
SELECT {bytxt}
{cols[{coln]},
n_datapts_cumul - n_datapts  + 1 AS ix |,
n_datapts_cumul AS ix nh
FROM (SELECT {bytxt)
{cols[coln)},
n_datapts,
# Calculate index ranges for each value within each by group
SUM(n_datapts) OVER ({f"PARTIVION GY (Dytxt.strip(",*)}" if part_cols else |
ORDER BY (cols{coln})) AS n_datapts_cumul
ese FROM value cts m) m
spark -sal(query) .createOrReplaceTempView( value ixrg')
me Dat em
------------------------------------------------------------------------------------------------------------------------
 @ Use indices to get median
tbfunc = if_two_middle_values.upper()
if not tofunc in('MEAN', "MIN', "MAX"):
tofunc = "MEAN
query = f"""
' SELECT {bytxt}
{tbfunc}(m.{cols{coln}}) AS {cols[coln}}
FROM value_ixrg m
INNER JOIN (SELECT {bytxt}
(MAX(ix_n)+1)/2 AS med_ix
FROM value_ixrg :
{f"GROUP BY {bytxt.strip(", ) ' if part_cols else  "}) x ON 1 = 1 {j_on}
WHERE m.ix_l <= CEILING(x.med_ix)  
AND m.ix_n >= FLOOR(x.med_ix)
("GROUP BY {bytxt.strip(',')}" if part_cols else ' |
spark.sql(query).createOrReplaceTempView(f' value med {colm}*)
@ Merge medians to one DF if needed, return summary OF
if len(cols) > 1:
jscr = 'value_med_@ m "\
+ * *,join(([ f  INNER JOIN value_med {coln) m(coin} OW 1 = 1"
+ '* join({f" AND m.(c} = m(coln).{c)" for   in part_cols])
for coln in range(1, len(cols))})
pewery ef" ""
J . SELECT (bytxt)
@.{cols[ }},
------------------------------------------------------------------------------------------------------------------------
nm. {cols(@]},
{', '.join([f'm{coln}.{cols[coln]}* for coln in range(1, len(cols))]))
FROM {jscr}
return spark.sql(query)
else: ap
return spark.sql("SELECT * FROM value_med_@")
def pivot_data(dt: DataFrame,
pivot: str,
cols: typing.Union[str, typing.List[(str}],
part_cols: typing.Optional(typing.Union[(str, typimg.tist{str}}] = None,
prefix: typing.Optional[str] = None,
replace_nulls: typing.Optional[typing.Union[str,, imt, flowt)|| = Nome) -> DataFrame:
Pivot values in "long" DataFrame across columns, with options to partition By other specified
'columns .
Input DataFrame is assumed to be unique on by+pivot column walwes. Piwot column can be string or
if integer, values are included in column names with prefix 'num' and megative signs replaced mith
of - Spark OF
pivot - String, name of pivot column
~ String or list of strings, columns with values to papulate into pivoted columns.
| parts cols - String or list of strings, column(s) to partition dy (optional)
prefix * String, prefix for pivoted column names (optional)   j
------------------------------------------------------------------------------------------------------------------------
prefix - String, prefix for pivoted column names (optional)
replace_nulls - String/Numeric, value to insert into pivote
Returns:
Spark DF
if isinstance(cols, str):
cols = [cols]
if part_cols is None:
part_cols = {}
elif isinstance(part_cols, str):
   part_cols = [part_cols]
if prefix is None:
prefix = '*
else:
prefix = f"{prefix.strip('_*)}_"
if replace_nulls is None:
replace_nulls == "NULL"
ehif isinstance(replace_nulls, str):
replace_nulls == f""{replace_nulls)*~
df .createOrReplaceTempView( 'df')
=", '.join((f"{c}  for c in part_cols])
'Af 'Let' in df. schema[pivot] datatype. simpleString():
ee eee 2 (Oo
Cee bat
id columns in place of nulls
for v in sorted(df.select(pivot) ,dropDuplicates(), wapandan() Lpswond. tolist()))  
as ake fen.
------------------------------------------------------------------------------------------------------------------------
Ee) vals = ((v, v) for v in sorted(df. select (pivot).dropDuplicates().toPandas()[pivot].tolist())]
py. sel = * " join([ ", NVL(MAX(CASE WHEN {pivot} == "{p}' THEN {c}) ELSE NULL END), {replace_nulls})"
ts fi + f" AS (c}_{prefix}{ps}"
for c in cols for p, ps in pv_vals])
query = "~~
SELECT {bytxt}
{pv_sel}
FROM df
GROUP BY {bytxt}
return spark.sql(query)
def unpivot_data(df: DataFrame,
  pivot: str,
cols: typing.Union[str, typing. List(str}},
part_cols: typing.Optional[(typing.Union[{str, typing. List{str])) = None
prefix: typing.Optional[str) = None) ~> OataPrame:
one
Un-pivot values in "wide" DataFrame to "long" format . the rewerse of pivot_data(). ,
Manes of column to be unpivoted snould follow the pattern <pivot> <prefix><pivot_value>, as with the  
| from pivot_data(). Pivot column will be set as integer type if values (from input colum names)
. with E ad
we amd can be converted to integers
------------------------------------------------------------------------------------------------------------------------
num_* and can be converted to integers
Inputs:
af - Spark OF
pivot - String, name of pivot column
cols - String or list of strings, columns with values to populate into pivoted columns.
part_cols - String or list of strings, column(s) to partition by (optional)
prefix - String, prefix for pivoted column names (optional)
Returns:
Spark OF
if isinstance(cols, str):
cols = [cols)
af part_cols is None:
part_cols = {]
elif isinstance(part_cols, str):
part_cols = [part_cols}
if prefix is None:
prefix =   "
else:
prefix = f*{prefix.strip('_'))_"
Of .createOrReplaceTempView('df')
bytxt = -Join({f"{c}" for   in part_cols})
  Get list of pivot values from column names
osu = wn] for cc in df columns if cc. startswitn(f'( )_ (prefix)")) for   in cols}
------------------------------------------------------------------------------------------------------------------------
pv_f = {]
for cn,   in enumerate(cols):
for cc in col_pv[cn]:
pyv = ccf[len(f*{c}_{prefix}'):]
if not(pvv in pv_f):
pv_f += [pvv]
@ Determine if pivot column is integer
py_int = min([{v.startswith('num_') for v in pv_f])
if pv_int:
try:
_ = [int(v[4:]-replace('neg', '-")) for wv in pw_f}
except ValueError:
pv_int = False
# Set up line for SQL SELECT statement(s) to recreate pivot columm
pv_sel = []
for c in cols:
1f pv_int:
stpos = len(f'{c}_{prefix}num_")+i
h py_sel ~= (f"CAST(REPLACE(SUBSTA({pivot), {stpos)), 'meg', *=") AS INT)*]
; ) Stpos = len(f'{c}_{prefix)')+1
py_sel += [f"SUBSTR({pivot), {stpos))")
Af lem(cols) == 1:
olf wey Brauen To unpivot, use the simple version
a ak
SE cence
------------------------------------------------------------------------------------------------------------------------
query = f"""
SELECT {bytxt},
{pv_sel[ ]} AS {pivot},
{cols[@]}
FROM df
UNPIVOT ({cols[ @]} FOR {pivot} IN ({", '-join(col_pv[@})}))
return spark.sql(query)
else:
# Define view for each unpivoted column
for c in range(len(cols)):
query = f"""
SELECT {bytxt},
{pv_sel(c]} AS {pivot},
{cols[(c}}
FROM df
UNPIVOT ({cols{c}} FOR (pivot) IN ({*, '.joim(col_gwie 
spark.sql(query).createOrReplaceTempView( f  unpyt(c}*)
  Get unique values for by and pivot columns
sa-
UNION ALL *.join((F'SELECT (bytxt), (pivot) PROM unpwt{c)* for c in range(len(cols))))
query = f"SELECT DISTINCT (bytxt), (pivot) FROM ({s@})"
spark .sqi(query).createOrReplacelempView( 'df out dase')
@ Combine ano return
1, Seana =", '.Join([f'b.(4}' for j in [*part_cols, pivot)
al as
Bask? Sitreletcl)" fore te rengncientoghad 21)
------------------------------------------------------------------------------------------------------------------------
=)
query"
SELECT {bytxt},
{pv_sel[c]} AS {pivot),
{cols[c}}
FROM af
UNPIVOT ({cols[c]} FOR {pivot} IN ({*, "-Join(col_py[c]})}))
spark.sql(query).createOrReplaceTempView(f"unpvt{c}*)
# Get unique values for by and pivot columns
sq =   UNION ALL  
join([f'SELECT (bytxt), (pivot) FROM unpwt{c}' for   im range(len(cols))})
' query = f"SELECT DISTINCT {bytxt}, {pivot} FROM ({sq})}~
spark.sql(query) .createOrReplaceTempView( 'df_out_base")
# Combine and return
seltxt = ', '.join([f'b.{j}  for j in [*part_cols, pivet};
+[f't{c}.{cols(c}]}* for   in range(lem(cals)}),;
jvuxt = " AND *.join([f'b.{j} = t<c>.{j)' for J in (*part_cols, pivot)}})
jointxt = * *.join(({f"LEFT JOIN unpvt{c} t{c} ON {fvtnt.replace{"<c>", str(c))}"
for c in range(len(cols))})
query = f"""
SELECT ({seltxt)
FROM df_out_base b
{jointxt)
| Meturn spark .sql(quequery 26777 "
SELECT {bytxt},
{pv_sel[c]} AS {pivot},
{cols[c]}
FROM df
UNPIVOT ({cols(c]} FOR {pivot} IN ({*, *-Jjoin(col_pv[c])}))
spark.sql(query).createOrReplaceTempView(f 'unpvt{c}")
@ Get unique values for by and pivot columns
sq = * UNION ALL '.join([f'SELECT {bytxt}, {pivot} FROM umpwt{c}' for   im range(len(cols))))
query = f"SELECT DISTINCT {bytxt}, {pivot} FROM ({sq}}"
spark.sql(query).createOrReplaceTempView( 'df_out_ base")
# Combine and return
seltxt =  , '.join([f'b.{j}' for J in (*part_cols, pivot})
+[f't(c}.{cols(c}}* for   in range(len(cols)}.5
jytxt = " AND '.join({f'b.(j) = t<e>.{j}* for 7 in [*part_cals, pivot)})
| Jointxt =   *.join([f"LEFT JOIN unpvt{c}) t(c}) ON (jvtxt.replace{*<c>*, str(c))}"
for c in range(len(cols))}}
query =  """
SELECT (seltxt)
FROM df_out_base b
{jointxt)
| feturn spark .sqi (query)
atic
------------------------------------------------------------------------------------------------------------------------
A
 "
451
=
fo  Users > g977209 > git > offer-opimization  
a
helpers
Name
a data_ext
@ _init_py
a azure_helpers.py
  jov_nelpers py
@ misc y
4 Std_imports.py
Man oP PY File
2 oes PY File
ac 6
------------------------------------------------------------------------------------------------------------------------
e
(pated Videos
9877208 > git > offer-optimization   helpers > data_ext
Name
@ _inn_py
a get_acds_dim py
4 get_all_division_codes py
@) ger_bec_elig.py
  get cfic py
P get_div_stores.py
@) get krweek dates py
@) get trailer_baskets. py
~ 0 earch data_en
Date modified
2024 12 10 PM
"W24 1_30 PM
ere
at
PY File
PY File
PY File
PY File
PY File
PY File
PY File
PY File
