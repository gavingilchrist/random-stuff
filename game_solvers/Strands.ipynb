{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3bc55d1-f37c-41be-b99e-a2abd5e180a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4618b844-c1b5-49cc-9fff-8cb827d1ee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = np.array(open('../data/scrabble_dict.txt', 'r').read().split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fad5343f-240d-4688-aeef-b266d0abf8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "puzzle = \"\"\"\n",
    "ACNARA\n",
    "YMTIOC\n",
    "TAGSCB\n",
    "JECIAT\n",
    "GUIOIC\n",
    "LGRMRE\n",
    "EDTACN\n",
    "RUEVAD\n",
    "\"\"\"\n",
    "n_wds = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f2a71c9-fb45-4f92-a8a6-27949c7a6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all words with 4+ letters in grid\n",
    "lgrid = np.array([list(r) for r in puzzle.split('\\n') if r != ''])\n",
    "grr, grc = lgrid.shape\n",
    "rr = np.repeat(np.arange(grr)+1, grc)\n",
    "cc = np.tile(np.arange(grc)+1, grr)\n",
    "lgrid = np.pad(lgrid, 1, constant_values='#')\n",
    "\n",
    "found_str = np.expand_dims(lgrid[rr, cc], 1)\n",
    "\n",
    "avail_grid = np.repeat(-np.ones(tuple([1, *lgrid.shape]), dtype=int), \n",
    "                       rr.shape[0], \n",
    "                       axis=0)\n",
    "avail_grid[np.arange(avail_grid.shape[0]), 1:-1, 1:-1] = 0\n",
    "avail_grid[np.arange(avail_grid.shape[0]), rr, cc] = 1\n",
    "\n",
    "found_wds = list()\n",
    "found_wds_map = list()\n",
    "lnum = 1\n",
    "\n",
    "while found_str.shape[0] > 0:\n",
    "    rr = np.repeat(rr, 8) + np.tile([-1, -1, 0, 1, 1, 1, 0, -1], rr.shape[0])\n",
    "    cc = np.repeat(cc, 8) + np.tile([0, 1, 1, 1, 0, -1, -1, -1], cc.shape[0])\n",
    "    avail_grid = np.repeat(avail_grid, 8, axis=0)\n",
    "    filt = avail_grid[np.arange(avail_grid.shape[0]), rr, cc] == 0\n",
    "    \n",
    "    rr, cc, avail_grid = [i[filt] for i in(rr, cc, avail_grid)]\n",
    "    found_str = np.column_stack((np.repeat(found_str, 8, axis=0)[filt], \n",
    "                                 lgrid[rr, cc]))\n",
    "    \n",
    "    fstr = found_str.reshape(-1).view(f'<U{found_str.shape[1]}')\n",
    "    lnum += 1\n",
    "    avail_grid[np.arange(avail_grid.shape[0]), rr, cc] = lnum\n",
    "    valid_str = np.unique(all_words.view('<U1')\n",
    "                                   .reshape(all_words.shape[0], -1)[:, :found_str.shape[1]]\n",
    "                                   .reshape(-1)\n",
    "                                   .view(f'<U{found_str.shape[1]}'))\n",
    "    \n",
    "    vstr_filt = np.isin(fstr, valid_str)\n",
    "    rr, cc, found_str, fstr, avail_grid = [i[vstr_filt] for i in(rr, cc, found_str, fstr, avail_grid)]\n",
    "\n",
    "    if lnum >= 4:\n",
    "        wordfilt = np.isin(fstr, all_words)\n",
    "        found_wds += fstr[wordfilt].tolist()\n",
    "        found_wds_map += [avail_grid[wordfilt][:, 1:-1, 1:-1]]\n",
    "\n",
    "found_wds = np.array(['', *found_wds])\n",
    "found_wds_map = np.concatenate([np.zeros_like(found_wds_map[0][:1]),\n",
    "                                *found_wds_map],\n",
    "                               axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c593b97-2c0c-4718-aa63-924a0fabea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_flood_chk(inarray):\n",
    "    flood_chk = np.pad(inarray, \n",
    "                       ((0, 0), (1, 1), (1, 1)), \n",
    "                       'constant', \n",
    "                       constant_values=9999)\n",
    "    while True:\n",
    "        n_rch = np.sum(flood_chk == -2)\n",
    "        flood_chk[:, 1:-1, 1:-1] = np.where(((flood_chk[:, 1:-1, 1:-1] <= 0)\n",
    "                                             & np.any(np.stack([flood_chk[:, \n",
    "                                                                1+r_os: flood_chk.shape[1]-1+r_os, \n",
    "                                                                1+c_os: flood_chk.shape[2]-1+c_os] == -2\n",
    "                                                                for r_os in range(-1, 2)\n",
    "                                                                for c_os in range(-1, 2)\n",
    "                                                                if r_os != 0 or c_os != 0]),\n",
    "                                                      axis=0)),\n",
    "                                            -2,\n",
    "                                            flood_chk[:, 1:-1, 1:-1])\n",
    "        if np.sum(flood_chk == -2) == n_rch:\n",
    "            break\n",
    "            \n",
    "    return flood_chk[:, 1:-1, 1:-1] == -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba27023-3998-4fcd-98df-fbb6b98876ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get all pairs of non-overlapping words in Pandas DF\n",
    "# x = np.repeat(np.arange(found_wds.shape[0]), found_wds.shape[0])\n",
    "# y = np.tile(np.arange(found_wds.shape[0]), found_wds.shape[0])\n",
    "# pair_ix = np.column_stack((x, y))[y > x]\n",
    "# for r in range(grr):\n",
    "#     for c in range(grc):\n",
    "#         pair_ix = pair_ix[np.any(found_wds_map[:, r, c][pair_ix] == 0, axis=1)]\n",
    "\n",
    "# # Get all sets of {n_wds-1} non-overlapping words\n",
    "# pair_ix = pair_ix.merge(pair_ix.rename(columns={'wd1': 'wd2'}), \n",
    "#                         on='wd0')\\\n",
    "#                  .merge(pair_ix.rename(columns={'wd0': 'wd1',\n",
    "#                                                 'wd1': 'wd2'}), \n",
    "#                         on=['wd1', 'wd2'])\n",
    "# pair_ix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d267ce5a-ec0a-453e-b2cb-c7cbaa6da095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poss_wds_ix = np.expand_dims(np.arange(found_wds.shape[0]), 1)\n",
    "# poss_map = np.copy(found_wds_map)\n",
    "\n",
    "# expix = np.arange(found_wds.shape[0]*poss_wds_ix.shape[0])\\\n",
    "#             [np.tile(np.arange(found_wds.shape[0]), poss_wds_ix.shape[0]) \n",
    "#              > np.repeat(poss_wds_ix[:, -1], found_wds.shape[0])]\n",
    "# possix = np.repeat(poss_wds_ix[:, -1], found_wds.shape[0])[expix]\n",
    "# addix = np.tile(np.arange(found_wds.shape[0]), poss_wds_ix.shape[0])[expix]\n",
    "# ovlp_filt = np.all((poss_map[possix] == 0) | (found_wds_map[addix] == 0), axis=(1, 2))\n",
    "\n",
    "# poss_wds_ix = np.column_stack((poss_wds_ix[possix][ovlp_filt], \n",
    "#                                np.arange(found_wds.shape[0])[addix][ovlp_filt]))\n",
    "# poss_map = poss_map[possix][ovlp_filt]\\\n",
    "#                + np.where(found_wds_map[addix][ovlp_filt] > 0,\n",
    "#                           found_wds_map[addix][ovlp_filt] + 10,\n",
    "#                           found_wds_map[addix][ovlp_filt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74b8f3d2-fb29-4ccd-bc5d-6a8e5214b69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 30\n",
      "0 1 79\n",
      "0 2 1187\n",
      "0 3 3389\n",
      "0 4 9926\n",
      "0 5 13359\n",
      "1 0 24075\n",
      "1 1 36346\n",
      "1 2 67047\n",
      "1 3 75200\n",
      "1 4 99998\n",
      "1 5 94522\n",
      "2 0 184157\n",
      "2 1 271258\n",
      "2 2 299586\n",
      "2 3 1090681\n",
      "2 4 1405278\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.46 GiB for an array with shape (1001737288,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(grc):\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Create indices for next word possibilities\u001b[39;00m\n\u001b[32m      9\u001b[39m     poss_ix = np.repeat(np.arange(poss_map.shape[\u001b[32m0\u001b[39m]),\n\u001b[32m     10\u001b[39m                         np.where(poss_map[:, rr, cc] == \u001b[32m0\u001b[39m, found_wds.shape[\u001b[32m0\u001b[39m], \u001b[32m1\u001b[39m))\n\u001b[32m     11\u001b[39m     nw_ix = np.arange(poss_ix.shape[\u001b[32m0\u001b[39m])\\\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m             - np.maximum.accumulate(np.where(poss_ix != \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposs_ix\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[32m     13\u001b[39m                                              np.arange(poss_ix.shape[\u001b[32m0\u001b[39m]), \n\u001b[32m     14\u001b[39m                                              \u001b[32m0\u001b[39m))\n\u001b[32m     15\u001b[39m     filt = (nw_ix == \u001b[32m0\u001b[39m) | (found_wds_map[nw_ix, rr, cc] != \u001b[32m0\u001b[39m)\n\u001b[32m     16\u001b[39m     poss_ix, nw_ix = (i[filt] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m (poss_ix, nw_ix))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\lib\\_function_base_impl.py:5711\u001b[39m, in \u001b[36mappend\u001b[39m\u001b[34m(arr, values, axis)\u001b[39m\n\u001b[32m   5709\u001b[39m     values = ravel(values)\n\u001b[32m   5710\u001b[39m     axis = arr.ndim-\u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5711\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 7.46 GiB for an array with shape (1001737288,) and data type int64"
     ]
    }
   ],
   "source": [
    "poss_wds = np.empty((1, 0), dtype=found_wds.dtype)\n",
    "poss_map = np.zeros_like(found_wds_map[:1])\n",
    "poss_wds_done = []\n",
    "poss_map_done = []\n",
    "\n",
    "for rr in range(grr):\n",
    "    for cc in range(grc):\n",
    "        # Create indices for next word possibilities\n",
    "        poss_ix = np.repeat(np.arange(poss_map.shape[0]),\n",
    "                            np.where(poss_map[:, rr, cc] == 0, found_wds.shape[0], 1))\n",
    "        nw_ix = np.arange(poss_ix.shape[0])\\\n",
    "                - np.maximum.accumulate(np.where(poss_ix != np.append([-1], poss_ix[:-1]), \n",
    "                                                 np.arange(poss_ix.shape[0]), \n",
    "                                                 0))\n",
    "        filt = (nw_ix == 0) | (found_wds_map[nw_ix, rr, cc] != 0)\n",
    "        poss_ix, nw_ix = (i[filt] for i in (poss_ix, nw_ix))\n",
    "        for r in range(grr):\n",
    "            for c in range(grc):\n",
    "                filt = (poss_map[poss_ix, r, c] == 0) | (found_wds_map[nw_ix, r, c] == 0)\n",
    "                poss_ix, nw_ix = (i[filt] for i in (poss_ix, nw_ix))\n",
    "\n",
    "        # Generate words list for each possible scenario\n",
    "        poss_wds = np.column_stack((poss_wds[poss_ix], found_wds[nw_ix]))\n",
    "        for c in range(poss_wds.shape[1]-2, -1, -1):\n",
    "            filt = poss_wds[:, c] == ''\n",
    "            poss_wds[filt, c:-1] = poss_wds[filt, c+1:]\n",
    "            poss_wds[filt, -1] = ''\n",
    "        poss_wds = poss_wds[:, :np.sum(np.sum(poss_wds != '', axis=0) > 0)]\n",
    "\n",
    "        # Count words for each possible scenario\n",
    "        poss_nwds = np.sum(poss_wds != '', axis=1)\n",
    "\n",
    "        # Generate grid map for each possible scenario\n",
    "        poss_map = poss_map[poss_ix]\\\n",
    "                   + np.where(found_wds_map[nw_ix] > 0, \n",
    "                              found_wds_map[nw_ix] + (np.expand_dims(poss_nwds, (1, 2)) * 100), \n",
    "                              found_wds_map[nw_ix])\n",
    "        poss_map[nw_ix == 0, rr, cc] = np.where(poss_map[nw_ix == 0, rr, cc] == 0, \n",
    "                                                -1, \n",
    "                                                poss_map[nw_ix == 0, rr, cc])\n",
    "\n",
    "        # Filter out scenarios where a -1 is orphaned, cannot be part of pangram\n",
    "        iso_from = dict()\n",
    "        iso_from['N'] = do_flood_chk(np.append(-2*np.ones_like(poss_map[:, :1, :]),\n",
    "                                               poss_map, \n",
    "                                               axis=1))[:, 1:, :]\n",
    "        iso_from['S'] = do_flood_chk(np.append(poss_map,\n",
    "                                               -2*np.ones_like(poss_map[:, :1, :]),\n",
    "                                               axis=1))[:, :-1, :]\n",
    "        iso_from['W'] = do_flood_chk(np.append(-2*np.ones_like(poss_map[:, :, :1]),\n",
    "                                               poss_map, \n",
    "                                               axis=2))[:, :, 1:]\n",
    "        iso_from['E'] = do_flood_chk(np.append(poss_map,\n",
    "                                               -2*np.ones_like(poss_map[:, :, :1]),\n",
    "                                               axis=2))[:, :, :-1]\n",
    "        filt = ~np.any((iso_from['N'] | iso_from['S']) & (iso_from['W'] | iso_from['E']), \n",
    "                       axis=(1, 2))\n",
    "        poss_wds = poss_wds[filt]\n",
    "        poss_nwds = poss_nwds[filt]\n",
    "        poss_map = poss_map[filt]        \n",
    "\n",
    "        # Remove scenarios where required number of words reached\n",
    "        filt = poss_nwds == n_wds\n",
    "        poss_wds_done += [poss_wds[filt]]\n",
    "        poss_map_done += [poss_map[filt]]\n",
    "        poss_wds = poss_wds[~filt]\n",
    "        poss_map = poss_map[~filt]\n",
    "\n",
    "        print(rr, cc, poss_wds.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43340ae5-ebf3-4649-95cd-e6ba7e97249f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[102, 101,  -1,  -1,  -1,  -1],\n",
       "       [ -1, 103,  -1,  -1,  -1,  -1],\n",
       "       [ -1, 104,  -1, 203,  -1,   0],\n",
       "       [  0,   0, 204,   0, 202,   0],\n",
       "       [  0,   0,   0, 205,   0, 201],\n",
       "       [  0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poss_map[1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcade96f-b92b-4cd7-a7e7-02c1014c9e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CAMA', 'CASCO', '', '', '', ''], dtype='<U10')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poss_wds[1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e37a59c-ed36-4d52-8514-172e88381b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ACTA', 'NISI', 'ACRO', 'GAMY', 'TEGU', 'CACTI'], dtype='<U10')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poss_wds_done[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41ae6585-0758-4789-b73c-af78605125f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[101, 102, 201, 104, 303, 301],\n",
       "       [404, 403, 103, 202, 304, 302],\n",
       "       [501, 402, 401, 203, 601,   0],\n",
       "       [  0, 502,   0, 204, 602, 604],\n",
       "       [503, 504,   0,   0, 605, 603],\n",
       "       [  0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poss_map_done[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead3231d-612e-4049-ac14-5bcd704bd6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 0 30\n",
    "# 0 1 83\n",
    "# 0 2 1541\n",
    "# 0 3 4999\n",
    "# 0 4 17595\n",
    "# 0 5 27904\n",
    "# 1 0 53133\n",
    "# 1 1 88391"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
